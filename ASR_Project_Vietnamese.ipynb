{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4653379,"sourceType":"datasetVersion","datasetId":2703147}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cài đặt các thư viện cần thiết \n!pip install --upgrade transformers datasets jiwer librosa evaluate matplotlib\n!pip install -U ipywidgets","metadata":{"_uuid":"34735e7a-5b6e-4b43-a262-546fb4ee1267","_cell_guid":"a17475d8-6169-42e9-a923-a7a44769ceb6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import các thư viện\nimport os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wave\nimport contextlib\nimport torch\nimport torchaudio\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nimport shutil\nimport re\nimport json\nimport evaluate  # Thay thế load_metric bằng evaluate\nfrom datasets import load_dataset, Audio, Dataset\nfrom transformers import (Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments,\n                          Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, DataCollatorWithPadding)\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n%matplotlib inline","metadata":{"_uuid":"8334062f-2998-46fd-913f-616c102b392f","_cell_guid":"a0e22073-3eb6-4c4a-b4a5-8f650b413a93","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kiểm tra thiết bị\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Sử dụng thiết bị: {device}\")\n\n# Đường dẫn đến dataset VIVOS\ntrain_audio_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/train/waves'\ntrain_prompts_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/train/prompts.txt'\ntrain_genders_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/train/genders.txt'\n\ntest_audio_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/waves'\ntest_prompts_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/prompts.txt'\ntest_genders_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/genders.txt'","metadata":{"_uuid":"30349767-af79-45a9-9912-c2fd0c9d462b","_cell_guid":"fbecfafa-fe40-47f9-915e-c18c30180697","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hàm để đọc file prompts.txt và trả về DataFrame\ndef load_prompts(prompts_path):\n    transcripts = []\n    with open(prompts_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            id, text = line.strip().split(' ', 1)\n            transcripts.append({'id': id, 'text': text.lower()})\n    return pd.DataFrame(transcripts)\n\n# Tạo DataFrame cho tập train và test\ntrain_transcripts = load_prompts(train_prompts_path)\ntest_transcripts = load_prompts(test_prompts_path)","metadata":{"_uuid":"ae4d2fcf-3388-4256-90df-0726eca404df","_cell_guid":"a2c05de0-06bc-4f47-b7ac-39353648ddde","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thêm đường dẫn âm thanh vào DataFrame\ndef get_audio_path(audio_base_path, audio_id):\n    speaker = audio_id.split('_')[0]\n    return os.path.join(audio_base_path, speaker, audio_id + '.wav')\n\ntrain_transcripts['audio'] = train_transcripts['id'].apply(lambda x: get_audio_path(train_audio_path, x))\ntest_transcripts['audio'] = test_transcripts['id'].apply(lambda x: get_audio_path(test_audio_path, x))","metadata":{"_uuid":"5be8d721-f696-4243-adca-2f5d8f2eb93d","_cell_guid":"e2349a94-7116-4766-ac94-df0773d011d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loại bỏ các ký tự đặc biệt và chuyển văn bản về chữ thường\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"“%‘”�]'\n\ndef remove_special_characters(batch):\n    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n    return batch\n\n# Áp dụng hàm tiền xử lý dữ liệu\ntrain_transcripts = train_transcripts.apply(remove_special_characters, axis=1)\ntest_transcripts = test_transcripts.apply(remove_special_characters, axis=1)","metadata":{"_uuid":"40f3ab0b-ae6e-4bad-973a-66b1bca8cfa7","_cell_guid":"60a2aded-ba42-4239-a358-17ddc7033792","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_dict = {\"ẻ\": 0, \"ẵ\": 1, \"k\": 2, \"ặ\": 3, \"d\": 4, \"õ\": 5, \"á\": 6, \"ở\": 7, \"s\": 8, \"ả\": 9, \"u\": 10, \"ừ\": 11, \"ử\": 12, \"ạ\": 13, \"ổ\": 14, \"â\": 15, \"ệ\": 16, \"ủ\": 17, \"ầ\": 18, \"e\": 19, \"ẳ\": 20, \"ỡ\": 21, \"v\": 22, \"ú\": 23, \"à\": 24, \"ù\": 25, \"m\": 26, \"ờ\": 27, \"ớ\": 28, \"ỵ\": 29, \"ắ\": 30, \"ấ\": 31, \"ó\": 32, \"y\": 33, \"ỏ\": 34, \"ỳ\": 35, \"ỷ\": 36, \"ê\": 37, \"ĩ\": 38, \"ậ\": 39, \"ợ\": 40, \"l\": 41, \"ố\": 42, \"ữ\": 43, \"ỗ\": 44, \"h\": 45, \"ẹ\": 46, \"ò\": 47, \"ộ\": 48, \"è\": 49, \"ơ\": 50, \"ồ\": 51, \"é\": 52, \"ế\": 53, \"ự\": 54, \"ô\": 55, \"c\": 56, \"n\": 57, \"ẽ\": 58, \"ă\": 59, \"ũ\": 60, \"ứ\": 61, \"ọ\": 62, \"ụ\": 63, \"ể\": 64, \"t\": 65, \"q\": 66, \"ý\": 67, \"í\": 68, \"ẩ\": 69, \"ề\": 70, \"ỉ\": 71, \"ư\": 72, \"r\": 73, \"ỹ\": 74, \"ị\": 75, \"ì\": 76, \"ằ\": 77, \"ã\": 78, \"đ\": 79, \"a\": 80, \"g\": 81, \"ễ\": 82, \"i\": 83, \"x\": 84, \"ẫ\": 85, \"b\": 87, \"o\": 88, \"p\": 89, \"|\": 86, \"[UNK]\": 90, \"[PAD]\": 91}","metadata":{"_uuid":"8a73ab93-1d0f-41c1-beb4-fc871b8bd955","_cell_guid":"ee5cb73b-bb93-47bb-8b93-8492cde6e649","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu vocabulary thành file JSON\nimport json\n\nwith open('vocab.json', 'w', encoding='utf-8') as vocab_file:\n    json.dump(vocab_dict, vocab_file, ensure_ascii=False)\n\n# Khởi tạo tokenizer\nvi_tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\", \n                                 unk_token=\"[UNK]\", \n                                 pad_token=\"[PAD]\", \n                                 word_delimiter_token=\"|\")","metadata":{"_uuid":"862841a0-30ea-4a81-bf06-e9d0bfee28f9","_cell_guid":"26272f08-9473-4926-8f59-116b4f6e902e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tải processor và model từ mô hình pre-trained\n\nfeature = Wav2Vec2FeatureExtractor.from_pretrained(\"CuongLD/wav2vec2-large-xlsr-vietnamese\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"CuongLD/wav2vec2-large-xlsr-vietnamese\", tokenizer = vi_tokenizer, feature_extractor = feature)\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\"CuongLD/wav2vec2-large-xlsr-vietnamese\", \n                                       attention_dropout=0.15,       # Tăng dropout đế giảm overfitting\n                                       hidden_dropout=0.15, activation_dropout=0.15,\n                                       ctc_loss_reduction=\"mean\",\n                                       pad_token_id = processor.tokenizer.pad_token_id)","metadata":{"_uuid":"bfcf5147-5404-4fad-b210-43473151a23d","_cell_guid":"61b00de9-7c2f-4274-b13d-ac12c1d96241","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chuyển đổi DataFrame thành Dataset của Hugging Face\ntrain_dataset = Dataset.from_pandas(train_transcripts)\ntest_dataset = Dataset.from_pandas(test_transcripts)\n\n# Chuyển cột 'audio' thành kiểu Audio\ntrain_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\ntest_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))","metadata":{"_uuid":"46f32165-113e-4465-b383-a57ee35e6684","_cell_guid":"61cf3fcb-a0b5-48e9-a691-672a5783f5db","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hàm tiền xử lý dữ liệu\ndef prepare_dataset(batch):\n    # Xử lý âm thanh\n    audio = batch[\"audio\"]\n    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n    # Xử lý văn bản\n    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n    return batch\n\n# Áp dụng hàm tiền xử lý dữ liệu\ntrain_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\ntest_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names)","metadata":{"_uuid":"cde2ed9c-27bd-4970-8f74-6b414730e358","_cell_guid":"a4be8f5e-e1e3-4f9f-924d-4f58a985ccd7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Tách inputs và labels\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        # Padding inputs\n        batch = self.processor.feature_extractor.pad(\n            input_features,\n            padding=self.padding,\n            return_tensors=\"pt\",\n        )\n\n        # Padding labels\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(\n                label_features,\n                padding=self.padding,\n                return_tensors=\"pt\",\n            )\n\n        # Thay thế giá trị padding bằng -100\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch\n\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"_uuid":"cf467bbd-f150-47b4-84ce-dda34cc10156","_cell_guid":"876e3c5c-b4e3-4ff2-b1ae-773827dd38a9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Định nghĩa hàm tính WER\nwer_metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    # Giải mã dự đoán\n    pred_str = processor.batch_decode(pred_ids)\n\n    # Giải mã nhãn\n    label_ids = pred.label_ids\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, group_tokens=False)\n\n    # Tính WER\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}","metadata":{"_uuid":"f53650e4-106e-4190-8c57-a261cd220db3","_cell_guid":"464acc3f-30d4-48ba-9861-6f1dfc01b420","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyTrainer:\n    def __init__(self, \n                 model = None, \n                 train_dataset = None,\n                 eval_dataset = None,\n                 processor = None,\n                 data_collator = None, \n                 learning_rate = 1e-3,\n                 weight_decay = 0.01,\n                 num_train_epochs = None, \n                 train_batch_size = None, \n                 gradient_accumulation_steps = 1,\n                 save_steps = 500,  \n                 logging_steps = 500, \n                 warmup_steps = 500,\n                 save_dir = \"./wav2vec2-vivos/checkpoints\", # path to folder to save result\n                 resume_from_checkpoint = False,\n                 checkpoint_dir = None): # path đến folder checkpoint, only use when resume_from_checkpoint = True\n        \n        self.model = model\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.processor = processor\n        self.data_collator = data_collator\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.num_train_epochs = num_train_epochs\n        self.train_batch_size = train_batch_size\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.save_steps = save_steps\n        self.logging_steps = logging_steps\n        self.warmup_steps = warmup_steps\n        self.save_dir = save_dir\n        self.checkpoint_dir = checkpoint_dir  \n        self.resume_from_checkpoint = resume_from_checkpoint\n        self.train_losses = []\n        self.eval_losses = []\n        self.learning_rate_values = []\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n        # Optimizer\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate,weight_decay = self.weight_decay)\n\n        self.start_epoch = 0\n        self.global_step = 0\n        if self.resume_from_checkpoint:\n            self.load_checkpoint()\n            \n    def train(self):\n        trainer_loader = DataLoader(self.train_dataset, batch_size = self.train_batch_size, collate_fn= self.data_collator, shuffle=True)\n        self.total_steps = len(trainer_loader) * self.num_train_epochs\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=self.learning_rate, total_steps=self.total_steps, pct_start=self.warmup_steps / self.total_steps)\n\n        for epoch in range(self.start_epoch, self.num_train_epochs):\n            self.model.train()\n            progress_bar = tqdm(trainer_loader, desc = f\"Epoch {epoch + 1}/{self.num_train_epochs}\", leave=False)\n            for step, batch in enumerate(progress_bar):\n                inputs = batch['input_values'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                outputs = self.model(inputs, labels = labels)\n                loss = outputs.loss\n                loss = loss / self.gradient_accumulation_steps # divide loss to simulate computing gradient over new_batch = batch * grad_accu_step\n                loss.backward()\n\n                if (step + 1) % self.gradient_accumulation_steps == 0:\n                    self.optimizer.step()\n                    self.scheduler.step()\n                    self.optimizer.zero_grad()\n                    self.global_step += 1\n\n                    if self.global_step % self.logging_steps == 0:\n                        progress_bar.set_postfix(loss = loss.item())\n\n                    if self.global_step % self.save_steps == 0:\n                        self.save_checkpoint(epoch, loss.item(), self.evaluate())\n                        \n            self.save_checkpoint(epoch + 1, loss.item(), self.evaluate())\n        \n    def evaluate(self):\n        self.model.eval()\n        eval_loader = DataLoader(self.eval_dataset, batch_size = self.train_batch_size, collate_fn = self.data_collator)\n        losses = []\n        for batch in tqdm(eval_loader, desc = \"Evaluating\", leave=False):\n            inputs = batch['input_values'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            with torch.no_grad():\n                outputs = self.model(inputs, labels=labels)\n            losses.append(outputs.loss.item())\n        avg_loss = np.mean(losses)\n        self.model.train()\n        return avg_loss\n\n    def save_checkpoint(self, epoch, train_loss, eval_loss):\n        os.makedirs(self.save_dir, exist_ok = True)\n        for item in os.listdir(self.save_dir):\n            item_path = os.path.join(self.save_dir, item)\n            # Remove all old files and folder, save only the lastest one\n            if os.path.isfile(item_path):\n                os.remove(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        checkpoint_path = os.path.join(self.save_dir, f\"checkpoint_epoch_{epoch}.pth\")\n        processor_path = os.path.join(self.save_dir, 'processor')\n        \n        self.train_losses.append(train_loss)\n        self.eval_losses.append(eval_loss)\n        \n        self.learning_rate_values.append(self.scheduler.get_last_lr()[0])\n        \n        torch.save({\n            'epoch': epoch,\n            'global_step': self.global_step,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_losses': self.train_losses,\n            'eval_losses': self.eval_losses,\n            'learning_rate_values': self.learning_rate_values,\n            'learning_rate': self.learning_rate,\n            'weight_decay': self.weight_decay,\n            'scheduler': self.scheduler,\n            'num_train_epochs': self.num_train_epochs, \n            'train_batch_size': self.train_batch_size, \n            'gradient_accumulation_steps': self.gradient_accumulation_steps,\n            'save_steps': self.save_steps,  \n            'logging_steps': self.logging_steps, \n            'warmup_steps': self.warmup_steps,\n        }, checkpoint_path)\n\n        self.processor.save_pretrained(processor_path)\n        print(f\"Checkpoint saved at {checkpoint_path}, step: {self.global_step}/{self.total_steps / self.gradient_accumulation_steps}\")\n        print(f\"Processor saved at {processor_path}\")\n            \n\n    def load_checkpoint(self):\n        # Tìm checkpoint gần nhất\n        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith('checkpoint_epoch_')]\n        if checkpoints:\n            lastest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split(\".\")[0]))\n            checkpoint_path = os.path.join(self.checkpoint_dir, lastest_checkpoint)\n            processor_path = os.path.join(self.checkpoint_dir, 'processor')\n            \n            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n            # load model's + optim's parameters\n            self.model.load_state_dict(checkpoint['model_state_dict'])\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n            # load tham số chạy mô hình\n            self.start_epoch = checkpoint['epoch']\n            self.global_step = checkpoint['global_step']\n            self.learning_rate = checkpoint['learning_rate']\n            self.weight_decay = checkpoint['weight_decay']\n            self.scheduler = checkpoint['scheduler']\n            self.num_train_epochs = checkpoint['num_train_epochs']\n            self.train_batch_size = checkpoint['train_batch_size']\n            self.gradient_accumulation_steps = checkpoint['gradient_accumulation_steps']\n            self.save_steps = checkpoint['save_steps']\n            self.logging_steps = checkpoint['logging_steps'] \n            self.warmup_steps = checkpoint['warmup_steps']\n\n            # load plotting values\n            self.train_losses = checkpoint['train_losses']\n            self.eval_losses = checkpoint['eval_losses']\n            self.learning_rate_values = checkpoint['learning_rate_values']\n\n            # load processor\n            self.processor = self.processor.from_pretrained(processor_path)\n            print(f\"Resumed from checkpoint at epoch {self.start_epoch}, global step {self.global_step}\")","metadata":{"_uuid":"4d3f444a-456b-44b9-a940-07b7b7b77488","_cell_guid":"06791fc7-399e-461c-85fd-06f3a77d482a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = MyTrainer(model = model,\n                    train_dataset = train_dataset,\n                    eval_dataset = test_dataset,\n                    processor = processor,\n                    data_collator = data_collator,\n                    learning_rate = 1e-4, \n                    num_train_epochs=30,\n                    train_batch_size=4,\n                    gradient_accumulation_steps=2)","metadata":{"_uuid":"74fd592a-d33b-401e-bf26-bd0ae7af0772","_cell_guid":"6f6e75ae-3217-40f1-9200-9767241ab20f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bắt đầu huấn luyện\ntrainer.train()","metadata":{"_uuid":"a5edaec9-aa6f-4d1d-adab-e737db99ac56","_cell_guid":"332e4217-4425-4919-a84c-b8ef2ae80004","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}