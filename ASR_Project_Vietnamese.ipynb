{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4653379,
          "sourceType": "datasetVersion",
          "datasetId": 2703147
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ASR Project Vietnamese",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vutl/Automatic-Speech-Recognition/blob/main/ASR_Project_Vietnamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import kagglehub\n",
        "kynthesis_vivos_vietnamese_speech_corpus_for_asr_path = kagglehub.dataset_download('kynthesis/vivos-vietnamese-speech-corpus-for-asr')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "0ksj6fAjqpnS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cài đặt các thư viện cần thiết\n",
        "!pip install --upgrade transformers datasets jiwer librosa evaluate matplotlib torchaudio seaborn"
      ],
      "metadata": {
        "trusted": true,
        "id": "oJFX3Xm6qpnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import các thư viện\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wave\n",
        "import contextlib\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "import evaluate  # Thay thế load_metric bằng evaluate\n",
        "from datasets import load_dataset, Audio, Dataset\n",
        "from transformers import (\n",
        "    Wav2Vec2ForCTC,\n",
        "    Wav2Vec2Processor,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    Wav2Vec2CTCTokenizer\n",
        ")\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-11T05:19:04.396465Z",
          "iopub.execute_input": "2024-11-11T05:19:04.397037Z",
          "iopub.status.idle": "2024-11-11T05:19:04.415523Z",
          "shell.execute_reply.started": "2024-11-11T05:19:04.396976Z",
          "shell.execute_reply": "2024-11-11T05:19:04.41426Z"
        },
        "trusted": true,
        "id": "vnLbL-llqpnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra thiết bị\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Sử dụng thiết bị: {device}\")\n",
        "\n",
        "# Đường dẫn đến dataset VIVOS\n",
        "train_audio_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/train/waves'\n",
        "train_prompts_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/train/prompts.txt'\n",
        "train_genders_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/train/genders.txt'\n",
        "\n",
        "test_audio_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/waves'\n",
        "test_prompts_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/prompts.txt'\n",
        "test_genders_path = '../input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/genders.txt'"
      ],
      "metadata": {
        "trusted": true,
        "id": "u65brXgFqpnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm để đọc file prompts.txt và trả về DataFrame\n",
        "def load_prompts(prompts_path):\n",
        "    transcripts = []\n",
        "    with open(prompts_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            id, text = line.strip().split(' ', 1)\n",
        "            transcripts.append({'id': id, 'text': text.lower()})\n",
        "    return pd.DataFrame(transcripts)\n",
        "\n",
        "# Tạo DataFrame cho tập train và test\n",
        "train_transcripts = load_prompts(train_prompts_path)\n",
        "test_transcripts = load_prompts(test_prompts_path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7N1axPYzqpnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thêm đường dẫn âm thanh vào DataFrame\n",
        "def get_audio_path(audio_base_path, audio_id):\n",
        "    speaker = audio_id.split('_')[0]\n",
        "    return os.path.join(audio_base_path, speaker, audio_id + '.wav')\n",
        "\n",
        "train_transcripts['audio'] = train_transcripts['id'].apply(lambda x: get_audio_path(train_audio_path, x))\n",
        "test_transcripts['audio'] = test_transcripts['id'].apply(lambda x: get_audio_path(test_audio_path, x))"
      ],
      "metadata": {
        "trusted": true,
        "id": "1s8GwzucqpnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loại bỏ các ký tự đặc biệt và chuyển văn bản về chữ thường\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"“%‘”�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
        "    return batch\n",
        "\n",
        "# Áp dụng hàm tiền xử lý dữ liệu\n",
        "train_transcripts = train_transcripts.apply(remove_special_characters, axis=1)\n",
        "test_transcripts = test_transcripts.apply(remove_special_characters, axis=1)\n",
        "\n",
        "# Kiểm tra một số mẫu text sau khi tiền xử lý\n",
        "print(\"=== Một Số Mẫu Text từ Tập Train Sau Khi Tiền Xử Lý ===\")\n",
        "for i in range(5):\n",
        "    sample = train_transcripts.iloc[i]['text']\n",
        "    print(f\"Mẫu {i+1}: {sample}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"=== Một Số Mẫu Text từ Tập Test Sau Khi Tiền Xử Lý ===\")\n",
        "for i in range(5):\n",
        "    sample = test_transcripts.iloc[i]['text']\n",
        "    print(f\"Mẫu {i+1}: {sample}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5Rv8wp3xqpnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_dict = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5, \"f\": 6, \"g\": 7, \"h\": 8, \"i\": 9, \"j\": 10, \"k\": 11, \"l\": 12, \"m\": 13, \"n\": 14, \"o\": 15, \"p\": 16, \"q\": 17, \"r\": 18, \"s\": 19, \"t\": 20, \"u\": 21, \"v\": 22, \"w\": 23, \"x\": 24, \"y\": 25, \"z\": 26, \"à\": 27, \"á\": 28, \"â\": 29, \"ã\": 30, \"è\": 31, \"é\": 32, \"ê\": 33, \"ì\": 34, \"í\": 35, \"ò\": 36, \"ó\": 37, \"ô\": 38, \"õ\": 39, \"ù\": 40, \"ú\": 41, \"ý\": 42, \"ă\": 43, \"đ\": 44, \"ĩ\": 45, \"ũ\": 46, \"ơ\": 47, \"ư\": 48, \"ạ\": 49, \"ả\": 50, \"ấ\": 51, \"ầ\": 52, \"ẩ\": 53, \"ẫ\": 54, \"ậ\": 55, \"ắ\": 56, \"ằ\": 57, \"ẳ\": 58, \"ẵ\": 59, \"ặ\": 60, \"ẹ\": 61, \"ẻ\": 62, \"ẽ\": 63, \"ế\": 64, \"ề\": 65, \"ể\": 66, \"ễ\": 67, \"ệ\": 68, \"ỉ\": 69, \"ị\": 70, \"ọ\": 71, \"ỏ\": 72, \"ố\": 73, \"ồ\": 74, \"ổ\": 75, \"ỗ\": 76, \"ộ\": 77, \"ớ\": 78, \"ờ\": 79, \"ở\": 80, \"ỡ\": 81, \"ợ\": 82, \"ụ\": 83, \"ủ\": 84, \"ứ\": 85, \"ừ\": 86, \"ử\": 87, \"ữ\": 88, \"ự\": 89, \"ỳ\": 90, \"ỵ\": 91, \"ỷ\": 92, \"ỹ\": 93, \"|\": 0, \"<bos>\": 94, \"<eos>\": 95, \"<unk>\": 96, \"<pad>\": 97}"
      ],
      "metadata": {
        "trusted": true,
        "id": "WwHZwKlWqpnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải processor và model từ mô hình pre-trained\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"CuongLD/wav2vec2-large-xlsr-vietnamese\")\n",
        "model.to(device)\n",
        "\n",
        "vi_tokenizer = Wav2Vec2CTCTokenizer(config.vi_vocab_file, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"CuongLD/wav2vec2-large-xlsr-vietnamese\", tokenizer = vi_tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "QV1xSFhjqpnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Kiểm tra vocab của tokenizer\n",
        "print(\"=== Một Số Tokens Từ Tokenizer ===\")\n",
        "vocab = processor.tokenizer.get_vocab()\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda item: item[1])\n",
        "\n",
        "for token, id in sorted_vocab[:100]:  # In ra 100 token đầu tiên\n",
        "    print(f\"Token: {token} - ID: {id}\")\n",
        "\n",
        "# 11. Kiểm tra mã hóa một số mẫu văn bản thủ công với processor\n",
        "print(\"=== Kiểm Tra Mã Hóa Thủ Công Với Processor ===\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    encoding = processor.tokenizer(text, padding=False, truncation=False, add_special_tokens=False)\n",
        "    input_ids = encoding['input_ids']\n",
        "    tokens = processor.tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "    print(f\"Mẫu {i+1}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Encoded IDs: {input_ids}\")\n",
        "    print(f\"Tokens: {' '.join(tokens)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 12. Kiểm tra mã hóa các ký tự riêng biệt\n",
        "print(\"=== Kiểm Tra Mã Hóa Các Ký Tự Riêng Biệt ===\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"Mẫu {i+1}: {text}\")\n",
        "    for char in text:\n",
        "        encoding = processor.tokenizer(char, add_special_tokens=False)\n",
        "        input_ids = encoding['input_ids']\n",
        "        tokens = processor.tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "        print(f\"Ký tự: '{char}' - ID: {input_ids} - Token: {tokens}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 13. Kiểm tra cấu hình của tokenizer\n",
        "print(\"=== Cấu Hình Tokenizer ===\")\n",
        "print(processor.tokenizer.config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "b1Ne_Fv0qpnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "LH0kJgBHqpnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuyển đổi DataFrame thành Dataset của Hugging Face\n",
        "train_dataset = Dataset.from_pandas(train_transcripts)\n",
        "test_dataset = Dataset.from_pandas(test_transcripts)\n",
        "\n",
        "# Chuyển cột 'audio' thành kiểu Audio\n",
        "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EdfNuzzfqpnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tiền xử lý dữ liệu\n",
        "def prepare_dataset(batch):\n",
        "    # Xử lý âm thanh\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    # Xử lý văn bản với tokenizer đã chuẩn hóa\n",
        "    encoding = processor.tokenizer(batch[\"text\"], padding=False, truncation=False, add_special_tokens=False)\n",
        "    batch[\"labels\"] = encoding[\"input_ids\"]\n",
        "    return batch\n",
        "\n",
        "# Áp dụng hàm tiền xử lý dữ liệu\n",
        "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\n",
        "test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kVd1V9V5qpnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Một số mẫu văn bản tiếng Việt\n",
        "sample_texts = [\n",
        "    \"khách sạn\",\n",
        "    \"chỉ bằng cách luôn nỗ lực thì cuối cùng bạn mới được đền đáp\",\n",
        "    \"trong số các quốc gia công nghiệp phát triển\",\n",
        "    \"anh đã nhìn thấy trong những lải nhải dông dài của nhu\",\n",
        "    \"khủng hoảng môi trường cần được ngăn chặn\"\n",
        "]\n",
        "\n",
        "print(\"=== Kiểm Tra Mã Hóa Thủ Công Với Tokenizer Độc Lập ===\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    encoding = processor.tokenizer(text, padding=False, truncation=False, add_special_tokens=False)\n",
        "    input_ids = encoding['input_ids']\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "    print(f\"Mẫu {i+1}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Encoded IDs: {input_ids}\")\n",
        "    print(f\"Tokens: {' '.join(tokens)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GQlAQySHqpnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kiểm tra một số mẫu labels từ tập train và test\n",
        "print(\"=== Mẫu Labels từ Tập Train ===\")\n",
        "for i in range(5):\n",
        "    sample = train_dataset[i]\n",
        "    labels = sample['labels']\n",
        "    # Chuyển đổi IDs sang tokens, bỏ qua các token padding\n",
        "    label_tokens = processor.tokenizer.convert_ids_to_tokens(labels, skip_special_tokens=True)\n",
        "    print(f\"Mẫu {i+1}:\")\n",
        "    print(f\"Labels IDs: {labels}\")\n",
        "    print(f\"Labels Tokens: {' '.join(label_tokens)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"=== Mẫu Labels từ Tập Test ===\")\n",
        "for i in range(5):\n",
        "    sample = test_dataset[i]\n",
        "    labels = sample['labels']\n",
        "    # Chuyển đổi IDs sang tokens, bỏ qua các token padding\n",
        "    label_tokens = processor.tokenizer.convert_ids_to_tokens(labels, skip_special_tokens=True)\n",
        "    print(f\"Mẫu {i+1}:\")\n",
        "    print(f\"Labels IDs: {labels}\")\n",
        "    print(f\"Labels Tokens: {' '.join(label_tokens)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "y4beaDv5qpnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # Tách inputs và labels\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Padding inputs\n",
        "        batch = self.processor.feature_extractor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Padding labels\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.tokenizer.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # Thay thế giá trị padding bằng -100\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Khởi tạo Data Collator\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "D7xAvOjoqpnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Định nghĩa hàm tính WER\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    # Giải mã dự đoán\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "\n",
        "    # Giải mã nhãn\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
        "\n",
        "    # Tính WER\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "trusted": true,
        "id": "CSdJVuQeqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Thiết lập tham số huấn luyện\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2-vivos\",\n",
        "    group_by_length=True,\n",
        "    per_device_train_batch_size=4,  # Giảm batch size nếu cần\n",
        "    gradient_accumulation_steps=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=10,  # Có thể điều chỉnh\n",
        "    fp16=True,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=5e-5,  # Giảm learning rate từ 1e-4 xuống 5e-5\n",
        "    warmup_steps=500,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Callback cho Early Stopping\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "# Khởi tạo Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=processor,  # Truyền toàn bộ processor\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7HwUvL7qqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bắt đầu huấn luyện\n",
        "trainer.train()\n",
        "\n",
        "# Lưu mô hình cuối cùng\n",
        "trainer.save_model(\"./wav2vec2-vivos\")\n",
        "# Lưu processor\n",
        "processor.save_pretrained(\"./wav2vec2-vivos\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "48VYK9byqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải lại processor và mô hình đã huấn luyện\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"./wav2vec2-vivos\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-vivos\")\n",
        "model.to(device)\n",
        "\n",
        "# Chuyển mô hình sang chế độ đánh giá\n",
        "model.eval()\n",
        "\n",
        "# In ra một số mẫu text từ tập train và test sau khi tiền xử lý\n",
        "print(\"=== Mẫu Text từ Tập Train ===\")\n",
        "for i in range(5):\n",
        "    sample = train_transcripts.iloc[i]['text']\n",
        "    print(f\"Mẫu {i+1}: {sample}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"=== Mẫu Text từ Tập Test ===\")\n",
        "for i in range(5):\n",
        "    sample = test_transcripts.iloc[i]['text']\n",
        "    print(f\"Mẫu {i+1}: {sample}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Kiểm tra lại vocab của tokenizer sau khi huấn luyện\n",
        "print(\"=== Vocab Sau Khi Huấn Luyện ===\")\n",
        "for i, (token, id) in enumerate(processor.tokenizer.get_vocab().items()):\n",
        "    if i < 100:\n",
        "        print(f\"Token: {token} - ID: {id}\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Chọn một file âm thanh từ tập test\n",
        "audio_file = test_transcripts['audio'].iloc[0]\n",
        "print(f\"Đang thử nghiệm với file âm thanh: {audio_file}\")\n",
        "\n",
        "# Đọc file âm thanh\n",
        "audio_input, sampling_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "# Tiền xử lý âm thanh\n",
        "input_values = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
        "\n",
        "# Dự đoán\n",
        "with torch.no_grad():\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "print(\"Kết quả nhận dạng giọng nói:\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HXIIMo9vqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đánh giá mô hình trên tập test\n",
        "def evaluate_model(dataset):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for sample in dataset:\n",
        "        audio = sample[\"input_values\"]\n",
        "        label = sample[\"labels\"]\n",
        "\n",
        "        # Chuyển input_values về tensor và di chuyển vào thiết bị\n",
        "        input_tensor = torch.tensor(audio).unsqueeze(0).to(device)\n",
        "\n",
        "        # Dự đoán\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor).logits\n",
        "\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)[0]\n",
        "        predictions.append(transcription)\n",
        "\n",
        "        # Giải mã nhãn\n",
        "        label_ids = label\n",
        "        label_ids = [id for id in label_ids if id != -100]\n",
        "        label_str = processor.tokenizer.decode(label_ids, skip_special_tokens=True)\n",
        "        references.append(label_str)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
        "    print(f\"WER trên tập test: {wer * 100:.2f} %\")\n",
        "\n",
        "# Chạy đánh giá\n",
        "evaluate_model(test_dataset)"
      ],
      "metadata": {
        "id": "qAMYyeL8qpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Lưu vocabulary thành file JSON\n",
        "# with open('vocab.json', 'w', encoding='utf-8') as vocab_file:\n",
        "#     json.dump(vocab_dict, vocab_file, ensure_ascii=False)\n",
        "\n",
        "# # Khởi tạo tokenizer\n",
        "# tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\",\n",
        "#                                  unk_token=\"<unk>\",\n",
        "#                                  pad_token=\"<pad>\",\n",
        "#                                  word_delimiter_token=\"|\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "RiiEHu3UqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# # Tạo danh sách tất cả các từ trong tập dữ liệu\n",
        "# all_words = ' '.join(full_dataset['text']).split()\n",
        "# word_counts = Counter(all_words)\n",
        "\n",
        "# # Số lượng từ duy nhất\n",
        "# num_unique_words = len(word_counts)\n",
        "# print(f\"Số lượng từ duy nhất trong tập dữ liệu: {num_unique_words}\")\n",
        "\n",
        "# # 10 từ xuất hiện nhiều nhất\n",
        "# most_common_words = word_counts.most_common(10)\n",
        "# print(\"10 từ xuất hiện nhiều nhất:\")\n",
        "# for word, count in most_common_words:\n",
        "#     print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "GkcIx_Q2qpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Đọc file genders.txt và thêm thông tin giới tính\n",
        "# def load_genders(genders_path):\n",
        "#     genders = {}\n",
        "#     with open(genders_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             speaker_id, gender = line.strip().split(' ')\n",
        "#             genders[speaker_id] = gender\n",
        "#     return genders"
      ],
      "metadata": {
        "trusted": true,
        "id": "kk1BJkRAqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tải thông tin giới tính cho tập train và test\n",
        "# train_genders = load_genders(train_genders_path)\n",
        "# test_genders = load_genders(test_genders_path)\n",
        "\n",
        "# # Thêm cột 'speaker' và 'gender' vào DataFrame\n",
        "# train_transcripts['speaker'] = train_transcripts['id'].apply(lambda x: x.split('_')[0])\n",
        "# test_transcripts['speaker'] = test_transcripts['id'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "# train_transcripts['gender'] = train_transcripts['speaker'].map(train_genders)\n",
        "# test_transcripts['gender'] = test_transcripts['speaker'].map(test_genders)\n",
        "\n",
        "# # Thêm cột 'dataset' để phân biệt tập train và test\n",
        "# train_transcripts['dataset'] = 'train'\n",
        "# test_transcripts['dataset'] = 'test'\n",
        "\n",
        "# # Kết hợp hai tập dữ liệu\n",
        "# full_dataset = pd.concat([train_transcripts, test_transcripts], ignore_index=True)\n",
        "\n",
        "# # Phân tích dữ liệu\n",
        "# # a. Kiểm tra số lượng mẫu\n",
        "# num_train_samples = len(train_transcripts)\n",
        "# num_test_samples = len(test_transcripts)\n",
        "# num_total_samples = len(full_dataset)\n",
        "\n",
        "# print(f\"Số lượng mẫu trong tập huấn luyện: {num_train_samples}\")\n",
        "# print(f\"Số lượng mẫu trong tập kiểm tra: {num_test_samples}\")\n",
        "# print(f\"Tổng số mẫu: {num_total_samples}\")\n",
        "\n",
        "# # b. Phân tích thời lượng âm thanh\n",
        "# def get_audio_duration(audio_path):\n",
        "#     with contextlib.closing(wave.open(audio_path,'r')) as f:\n",
        "#         frames = f.getnframes()\n",
        "#         rate = f.getframerate()\n",
        "#         duration = frames / float(rate)\n",
        "#         return duration\n",
        "\n",
        "# # Thêm cột 'duration' vào DataFrame\n",
        "# full_dataset['duration'] = full_dataset['audio'].apply(get_audio_duration)\n",
        "\n",
        "# # Vẽ biểu đồ phân phối thời lượng âm thanh\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(full_dataset['duration'], bins=50, kde=True)\n",
        "# plt.title('Phân phối thời lượng âm thanh')\n",
        "# plt.xlabel('Thời lượng (giây)')\n",
        "# plt.ylabel('Số lượng mẫu')\n",
        "# plt.show()\n",
        "\n",
        "# # c. Phân tích số lượng từ trong transcript\n",
        "# def count_words(text):\n",
        "#     return len(text.split())\n",
        "\n",
        "# # Thêm cột 'num_words' vào DataFrame\n",
        "# full_dataset['num_words'] = full_dataset['text'].apply(count_words)\n",
        "\n",
        "# # Vẽ biểu đồ phân phối số lượng từ\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(full_dataset['num_words'], bins=30, kde=True)\n",
        "# plt.title('Phân phối số lượng từ trong transcript')\n",
        "# plt.xlabel('Số lượng từ')\n",
        "# plt.ylabel('Số lượng mẫu')\n",
        "# plt.show()\n",
        "\n",
        "# # d. Phân tích sóng âm của một mẫu\n",
        "# # Chọn một mẫu ngẫu nhiên\n",
        "# sample = full_dataset.sample(1).iloc[0]\n",
        "# audio_path_sample = sample['audio']\n",
        "# text_sample = sample['text']\n",
        "\n",
        "# # Đọc âm thanh\n",
        "# audio_data_sample, sr_sample = librosa.load(audio_path_sample, sr=None)\n",
        "\n",
        "# # Vẽ sóng âm\n",
        "# plt.figure(figsize=(14, 5))\n",
        "# librosa.display.waveshow(audio_data_sample, sr=sr_sample)\n",
        "# plt.title(f\"Sóng âm của mẫu: {sample['id']}\")\n",
        "# plt.xlabel('Thời gian (giây)')\n",
        "# plt.ylabel('Biên độ')\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"Transcript: {text_sample}\")\n",
        "\n",
        "# # e. Phân tích phân bố giọng nam và nữ\n",
        "# gender_counts = full_dataset['gender'].value_counts()\n",
        "# print(\"Số lượng mẫu theo giới tính:\")\n",
        "# print(gender_counts)\n",
        "\n",
        "# # Vẽ biểu đồ phân bố giới tính\n",
        "# plt.figure(figsize=(6, 6))\n",
        "# sns.countplot(x='gender', data=full_dataset)\n",
        "# plt.title('Phân bố giới tính trong tập dữ liệu')\n",
        "# plt.xlabel('Giới tính')\n",
        "# plt.ylabel('Số lượng mẫu')\n",
        "# plt.show()\n",
        "\n",
        "# # Phân tích thời lượng âm thanh theo giới tính\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.kdeplot(data=full_dataset, x='duration', hue='gender', fill=True)\n",
        "# plt.title('Phân phối thời lượng âm thanh theo giới tính')\n",
        "# plt.xlabel('Thời lượng (giây)')\n",
        "# plt.ylabel('Mật độ')\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "JBim-fb5qpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Lọc các mẫu có thời lượng từ 1 đến 7 giây\n",
        "# filtered_dataset = full_dataset[(full_dataset['duration'] >= 1.0) & (full_dataset['duration'] <= 7.0)]\n",
        "# print(f\"Số lượng mẫu sau khi lọc: {len(filtered_dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "T9VqOBkrqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(x='duration', y='num_words', data=full_dataset)\n",
        "# plt.title('Mối quan hệ giữa thời lượng âm thanh và số lượng từ')\n",
        "# plt.xlabel('Thời lượng (giây)')\n",
        "# plt.ylabel('Số lượng từ')\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ffFTazIRqpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tải thông tin giới tính cho tập train và test\n",
        "# train_genders = load_genders(train_genders_path)\n",
        "# test_genders = load_genders(test_genders_path)\n",
        "\n",
        "# # Thêm cột 'speaker' và 'gender' vào DataFrame\n",
        "# train_transcripts['speaker'] = train_transcripts['id'].apply(lambda x: x.split('_')[0])\n",
        "# test_transcripts['speaker'] = test_transcripts['id'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "# train_transcripts['gender'] = train_transcripts['speaker'].map(train_genders)\n",
        "# test_transcripts['gender'] = test_transcripts['speaker'].map(test_genders)\n",
        "\n",
        "# # Thêm cột 'dataset' để phân biệt tập train và test\n",
        "# train_transcripts['dataset'] = 'train'\n",
        "# test_transcripts['dataset'] = 'test'\n",
        "\n",
        "# # Thêm cột 'duration' (thời lượng âm thanh)\n",
        "# def get_audio_duration(audio_path):\n",
        "#     with contextlib.closing(wave.open(audio_path,'r')) as f:\n",
        "#         frames = f.getnframes()\n",
        "#         rate = f.getframerate()\n",
        "#         duration = frames / float(rate)\n",
        "#         return duration\n",
        "\n",
        "# train_transcripts['duration'] = train_transcripts['audio'].apply(get_audio_duration)\n",
        "# test_transcripts['duration'] = test_transcripts['audio'].apply(get_audio_duration)\n",
        "\n",
        "# # Thêm cột 'num_words' (số lượng từ trong transcript)\n",
        "# def count_words(text):\n",
        "#     return len(text.split())\n",
        "\n",
        "# train_transcripts['num_words'] = train_transcripts['text'].apply(count_words)\n",
        "# test_transcripts['num_words'] = test_transcripts['text'].apply(count_words)\n",
        "\n",
        "# # So sánh tập train và test\n",
        "\n",
        "# # 1. So sánh phân phối thời lượng âm thanh\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.kdeplot(data=train_transcripts, x='duration', label='Train', fill=True)\n",
        "# sns.kdeplot(data=test_transcripts, x='duration', label='Test', fill=True)\n",
        "# plt.title('Phân phối thời lượng âm thanh giữa tập Train và Test')\n",
        "# plt.xlabel('Thời lượng (giây)')\n",
        "# plt.ylabel('Mật độ')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # 2. So sánh phân phối số lượng từ trong transcript\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.kdeplot(data=train_transcripts, x='num_words', label='Train', fill=True)\n",
        "# sns.kdeplot(data=test_transcripts, x='num_words', label='Test', fill=True)\n",
        "# plt.title('Phân phối số lượng từ trong transcript giữa tập Train và Test')\n",
        "# plt.xlabel('Số lượng từ')\n",
        "# plt.ylabel('Mật độ')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # 3. So sánh phân bố giới tính\n",
        "# gender_counts_train = train_transcripts['gender'].value_counts()\n",
        "# gender_counts_test = test_transcripts['gender'].value_counts()\n",
        "\n",
        "# gender_df = pd.DataFrame({\n",
        "#     'train': gender_counts_train,\n",
        "#     'test': gender_counts_test\n",
        "# }).transpose()\n",
        "\n",
        "# gender_df.plot(kind='bar', figsize=(8, 6))\n",
        "# plt.title('Phân bố giới tính giữa tập Train và Test')\n",
        "# plt.xlabel('Dataset')\n",
        "# plt.ylabel('Số lượng mẫu')\n",
        "# plt.legend(title='Giới tính')\n",
        "# plt.show()\n",
        "\n",
        "# # 4. So sánh danh sách speaker giữa tập train và test\n",
        "# speakers_train = set(train_transcripts['speaker'])\n",
        "# speakers_test = set(test_transcripts['speaker'])\n",
        "\n",
        "# # Số lượng speaker trong mỗi tập\n",
        "# print(f\"Số lượng speaker trong tập Train: {len(speakers_train)}\")\n",
        "# print(f\"Số lượng speaker trong tập Test: {len(speakers_test)}\")\n",
        "\n",
        "# # Kiểm tra xem có sự trùng lặp speaker giữa hai tập không\n",
        "# common_speakers = speakers_train.intersection(speakers_test)\n",
        "# print(f\"Số lượng speaker có mặt ở cả hai tập: {len(common_speakers)}\")\n",
        "\n",
        "# # Nếu cần, liệt kê các speaker chung\n",
        "# if len(common_speakers) > 0:\n",
        "#     print(\"Danh sách speaker có mặt ở cả hai tập:\")\n",
        "#     print(common_speakers)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dWbb5wFKqpnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}